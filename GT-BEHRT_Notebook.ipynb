{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9GsCbbcGPtk5"
      },
      "outputs": [],
      "source": [
        "!pip install import_ipynb\n",
        "!pip install -U -q PyDrive\n",
        "!pip install pytorch_pretrained_bert\n",
        "!pip install sparse\n",
        "!pip install transformers\n",
        "!pip install torchmetrics\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ugRxpSPRKkhx"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S2fZi2kYP9em"
      },
      "outputs": [],
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import numpy as np\n",
        "import sparse\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.nn as tgmnn\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.loader import DataListLoader as GraphLoader\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "import copy\n",
        "import sklearn.metrics as skm\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import pytorch_pretrained_bert as Bert\n",
        "import itertools\n",
        "from einops import rearrange, repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0m28BcCGQT7j"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from typing import Optional, Tuple, Union\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "from torch_geometric.typing import Adj, OptTensor, PairTensor, SparseTensor\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn import LayerNorm\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "class TransformerConv(MessagePassing):\n",
        "    _alpha: OptTensor\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: Union[int, Tuple[int, int]],\n",
        "        out_channels: int,\n",
        "        heads: int = 1,\n",
        "        concat: bool = True,\n",
        "        beta: bool = False,\n",
        "        dropout: float = 0.,\n",
        "        edge_dim: Optional[int] = None,\n",
        "        bias: bool = True,\n",
        "        root_weight: bool = True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.beta = beta and root_weight\n",
        "        self.root_weight = root_weight\n",
        "        self.concat = concat\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "        self._alpha = None\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
        "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
        "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
        "        self.layernorm1 = LayerNorm(out_channels)\n",
        "        self.layernorm2 = LayerNorm(out_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.proj = Linear(heads * out_channels, out_channels)\n",
        "        self.ffn = Linear(out_channels, out_channels)\n",
        "        self.ffn2 = Linear(out_channels, out_channels)\n",
        "        if edge_dim is not None:\n",
        "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
        "        else:\n",
        "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
        "\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()\n",
        "        self.lin_key.reset_parameters()\n",
        "        self.lin_query.reset_parameters()\n",
        "        self.lin_value.reset_parameters()\n",
        "        if self.edge_dim:\n",
        "            self.lin_edge.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, batch=None, return_attention_weights=None):\n",
        "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
        "        r\"\"\"Runs the forward pass of the module.\n",
        "\n",
        "        Args:\n",
        "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
        "                will additionally return the tuple\n",
        "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
        "                attention weights for each edge. (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "\n",
        "        H, C = self.heads, self.out_channels\n",
        "        residual = x\n",
        "        x = self.layernorm1(x, batch)\n",
        "        if isinstance(x, Tensor):\n",
        "            x: PairTensor = (x, x)\n",
        "        query = self.lin_query(x[1]).view(-1, H, C)\n",
        "        key = self.lin_key(x[0]).view(-1, H, C)\n",
        "        value = self.lin_value(x[0]).view(-1, H, C)\n",
        "        # propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\n",
        "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
        "                             edge_attr=edge_attr, size=None)\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None\n",
        "        if self.concat:\n",
        "            out = self.proj(out.view(-1, self.heads * self.out_channels))\n",
        "        else:\n",
        "            out = out.mean(dim=1)\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        out = out+residual\n",
        "        residual = out\n",
        "\n",
        "        out = self.layernorm2(out)\n",
        "        out = self.gelu(self.ffn(out))\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        out = self.ffn2(out)\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        out = out + residual\n",
        "        if isinstance(return_attention_weights, bool):\n",
        "            assert alpha is not None\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                return out, (edge_index, alpha)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                return out, edge_index.set_value(alpha, layout='coo')\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
        "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
        "                size_i: Optional[int]) -> Tensor:\n",
        "\n",
        "\n",
        "        if self.lin_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,\n",
        "                                                      self.out_channels)\n",
        "            key_j = key_j + edge_attr\n",
        "\n",
        "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)\n",
        "        self._alpha = alpha\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        out = value_j\n",
        "        if edge_attr is not None:\n",
        "            out = out + edge_attr\n",
        "\n",
        "        out = out * alpha.view(-1, self.heads, 1)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, heads={self.heads})')\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = tgmnn.Sequential('x, edge_index, edge_attr, batch', [\n",
        "            (TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True), 'x, edge_index, edge_attr -> x'),\n",
        "            nn.GELU(),\n",
        "            (TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True), 'x, edge_index, edge_attr -> x'),\n",
        "            nn.GELU(),\n",
        "            (TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=False), 'x, edge_index, edge_attr -> x'),\n",
        "        ])\n",
        "\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size // 5)\n",
        "        self.embed_ee = nn.Embedding(7, config.hidden_size // 5)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_index_readout, edge_attr, batch):\n",
        "        indices = (x==0).nonzero().squeeze()\n",
        "        h_nodes = self.conv(self.embed(x), edge_index, self.embed_ee(edge_attr), batch)\n",
        "        x = h_nodes[indices]\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, segment, age\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        #self.word_embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
        "        self.word_embeddings = GraphTransformer(config)\n",
        "        self.type_embeddings = nn.Embedding(11, config.hidden_size//5, padding_idx=0)\n",
        "\n",
        "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size//5). \\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(config.age_vocab_size, config.hidden_size//5))\n",
        "\n",
        "        self.time_embeddings = nn.Embedding(367, config.hidden_size//5). \\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(367, config.hidden_size//5))\n",
        "\n",
        "        self.delta_embeddings = nn.Embedding(config.delta_size, config.hidden_size//5). \\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(config.delta_size, config.hidden_size//5))\n",
        "\n",
        "        self.los_embeddings = nn.Embedding(1192, config.hidden_size//5). \\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(1192, config.hidden_size//5))\n",
        "\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size//5). \\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size//5))\n",
        "\n",
        "\n",
        "\n",
        "        self.seq_layers = nn.Sequential(\n",
        "            nn.LayerNorm(config.hidden_size),\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
        "        self.acti = nn.GELU()\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
        "\n",
        "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, delta_ids, type_ids, posi_ids, los):\n",
        "        word_embed = self.word_embeddings(nodes, edge_index, edge_index_readout, edge_attr, batch)\n",
        "        type_embeddings = self.type_embeddings(type_ids)\n",
        "        age_embed = self.age_embeddings(age_ids)\n",
        "        los_embed = self.los_embeddings(los)\n",
        "\n",
        "        time_embeddings = self.time_embeddings(time_ids)\n",
        "        delta_embeddings = self.delta_embeddings(delta_ids)\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "\n",
        "        word_embed = torch.reshape(word_embed, type_embeddings.shape)\n",
        "        embeddings = torch.cat((word_embed, type_embeddings, posi_embeddings, age_embed, time_embeddings), dim=2)\n",
        "        b, n, _ = embeddings.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = self.seq_layers(embeddings)\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos / (10000 ** (2 * idx / hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos / (10000 ** (2 * idx / hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "#%%\n",
        "\n",
        "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config=config)\n",
        "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
        "        self.pooler = Bert.modeling.BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, delta_ids, type_ids, posi_ids, attention_mask=None, los=None,\n",
        "                output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(age_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, delta_ids, type_ids, posi_ids, los)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "class BertForMTR(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMTR, self).__init__(config)\n",
        "        self.num_labels = 1\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        #self.gru = nn.GRU(config.hidden_size, config.hidden_size // 2, 1, batch_first = True, bidirectional=True)\n",
        "        #self.gru = nn.Linear(config.hidden_size * 50, config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.apply(self.init_bert_weights)\n",
        "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, delta_ids, type_ids, posi_ids, attention_mask=None, labels=None, masks=None, los=None):\n",
        "        _, pooled_output = self.bert(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, delta_ids, type_ids, posi_ids, attention_mask,los,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        #pooled_output = self.dropout(pooled_output)\n",
        "        #pooled_output = pooled_output * attention_mask.unsqueeze(-1)\n",
        "        #pooled_output = torch.sum(pooled_output, axis=1) / torch.sum(attention_mask, axis=1).unsqueeze(-1)\n",
        "        #pooled_output = torch.mean(_, axis=1)\n",
        "        #pooled_output, x = self.gru(pooled_output)\n",
        "        #pooled_output = self.gru(torch.flatten(pooled_output, start_dim=1))\n",
        "        #pooled_output = self.relu(self.dropout(pooled_output))\n",
        "        logits = self.classifier(pooled_output).squeeze(dim=1)\n",
        "        bce_logits_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        discr_supervised_loss = bce_logits_loss(logits, labels)\n",
        "\n",
        "        return discr_supervised_loss, logits\n",
        "\n",
        "#%%\n",
        "\n",
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings = config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "        self.delta_size = config.get('delta_size')\n",
        "        self.graph_dropout_prob = config.get('graph_dropout_prob')\n",
        "\n",
        "class TrainConfig(object):\n",
        "    def __init__(self, config):\n",
        "        self.batch_size = config.get('batch_size')\n",
        "        self.use_cuda = config.get('use_cuda')\n",
        "        self.max_len_seq = config.get('max_len_seq')\n",
        "        self.train_loader_workers = config.get('train_loader_workers')\n",
        "        self.test_loader_workers = config.get('test_loader_workers')\n",
        "        self.device = config.get('device')\n",
        "        self.output_dir = config.get('output_dir')\n",
        "        self.output_name = config.get('output_name')\n",
        "        self.best_name = config.get('best_name')\n",
        "\n",
        "#%%\n",
        "\n",
        "class GDSet(Dataset):\n",
        "    def __init__(self, g):\n",
        "        self.g = g\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        g = self.g[index]\n",
        "        for i in range(len(g)):\n",
        "          g[i]['posi_ids'] = i\n",
        "        return g\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vSsg3bZFQngf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/GANBEHRT/final_data/new/data', 'rb') as handle:\n",
        "    dataset = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wbogAGQ5Q7X5"
      },
      "outputs": [],
      "source": [
        "train_l = int(len(dataset)*0.70)\n",
        "val_l = int(len(dataset)*0.10)\n",
        "test_l = len(dataset) - val_l - train_l\n",
        "number_output = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QkzMDTX9RCvl"
      },
      "outputs": [],
      "source": [
        "file_config = {\n",
        "    'model_path': 'model/', # where to save model\n",
        "    'model_name': 'CVDTransformer', # model name\n",
        "    'file_name': 'log.txt',  # log path\n",
        "}\n",
        "#create_folder(file_config['model_path'])\n",
        "\n",
        "global_params = {\n",
        "    'max_seq_len': 50,\n",
        "    'month': 1,\n",
        "    'gradient_accumulation_steps': 1\n",
        "}\n",
        "\n",
        "optim_param = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "train_params = {\n",
        "    'batch_size': 64,\n",
        "    'use_cuda': True,\n",
        "    'max_len_seq': global_params['max_seq_len'],\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'data_len' : len(dataset),\n",
        "    'train_data_len' : train_l,\n",
        "    'val_data_len' : val_l,\n",
        "    'test_data_len' : test_l,\n",
        "    'epochs' : 30,\n",
        "    'action' : 'train'\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    'vocab_size': 7204, # number of disease + symbols for word embedding\n",
        "    'hidden_size': 108*5, # word embedding and seg embedding hidden size\n",
        "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
        "    'age_vocab_size': 103, # number of vocab for age embedding\n",
        "    'delta_size': 144, # number of vocab for age embedding\n",
        "    'gender_vocab_size': 2,\n",
        "    'ethnicity_vocab_size': 2,\n",
        "    'race_vocab_size': 6,\n",
        "    'num_labels':1,\n",
        "    'feature_dict':7204,\n",
        "    'max_position_embedding': train_params['max_len_seq'], # maximum number of tokens\n",
        "    'hidden_dropout_prob': 0.2, # dropout rate\n",
        "    'graph_dropout_prob': 0.2, # dropout rate\n",
        "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
        "    'num_attention_heads': 12, # number of attention heads\n",
        "    'attention_probs_dropout_prob': 0.2, # multi-head attention dropout rate\n",
        "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'initializer_range': 0.02, # parameter weight initializer range\n",
        "    'number_output' : number_output,\n",
        "    'n_layers' : 3 - 1,\n",
        "    'alpha' : 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oo5093m5RPrH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "rr=1\n",
        "rs = ShuffleSplit(n_splits=1, test_size=.20, random_state=rr)\n",
        "\n",
        "k = 5\n",
        "few_shots = 0.05\n",
        "\n",
        "for i, (train_index_tmp, test_index) in enumerate(rs.split(dataset)):\n",
        "  rs2 = ShuffleSplit(n_splits=1, test_size=.125, random_state=rr)\n",
        "  for j, (train_index, val_index) in enumerate(rs2.split(train_index_tmp)):\n",
        "    train_index = train_index_tmp[train_index]\n",
        "    if few_shots < 1:\n",
        "      train_index = random.sample(list(train_index), int(len(train_index) * few_shots))\n",
        "    val_index = train_index_tmp[val_index]\n",
        "\n",
        "    trainDSet = [dataset[x] for x in train_index]\n",
        "    valDSet = [dataset[x] for x in val_index]\n",
        "    testDSet = [dataset[x] for x in test_index]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hI6_6l2mRiJx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "conf = BertConfig(model_config)\n",
        "behrt = BertForMTR(conf)\n",
        "\n",
        "behrt = behrt.to(train_params['device'])\n",
        "\n",
        "#models parameters\n",
        "transformer_vars = [i for i in behrt.parameters()]\n",
        "\n",
        "#optimizer\n",
        "import transformers\n",
        "optim_behrt = torch.optim.AdamW(transformer_vars, lr=3e-5)\n",
        "#sched = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optim_behrt, 1000, 500*train_params['epochs'], 4, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zzl7TSDhRk2v"
      },
      "outputs": [],
      "source": [
        "def run_epoch(e, trainload, device):\n",
        "    tr_loss = 0\n",
        "    start = time.time()\n",
        "    behrt.train()\n",
        "    for step, data in enumerate(trainload):\n",
        "        optim_behrt.zero_grad()\n",
        "\n",
        "        batched_data = Batch()\n",
        "        graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
        "        graph_batch = graph_batch.to(device)\n",
        "        nodes = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        edge_index_readout = graph_batch.edge_index\n",
        "        edge_attr = graph_batch.edge_attr\n",
        "        batch = graph_batch.batch\n",
        "        age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
        "        time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
        "        delta_ids = torch.reshape(graph_batch.delta, [graph_batch.delta.shape[0] // 50, 50])\n",
        "        type_ids = torch.reshape(graph_batch.adm_type, [graph_batch.adm_type.shape[0] // 50, 50])\n",
        "        posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
        "        attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
        "        attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
        "        los = torch.reshape(graph_batch.los, [graph_batch.los.shape[0] // 50, 50])\n",
        "\n",
        "        labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
        "        masks = torch.reshape(graph_batch.mask, [graph_batch.mask.shape[0] // 50, 50])[:, 0]\n",
        "        loss, logits = behrt(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids,delta_ids,type_ids,posi_ids,attMask, labels, masks, los)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] >1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        if step%500 == 0:\n",
        "            print(loss.item())\n",
        "        optim_behrt.step()\n",
        "        #sched.step()\n",
        "        del loss\n",
        "        #result = result + torch.sum(torch.sum(torch.mul(torch.abs(torch.subtract(pred, label)), target_mask), dim = 0)).cpu()\n",
        "        #sum_labels = sum_labels + torch.sum(target_mask, dim=0).cpu()\n",
        "    #print(result / sum_labels)\n",
        "    cost = time.time() - start\n",
        "    return tr_loss, cost\n",
        "#%%\n",
        "\n",
        "def train(trainload, valload, device):\n",
        "    with open(\"v_behrt_log_train.txt\", 'w') as f:\n",
        "            f.write('')\n",
        "    best_val = math.inf\n",
        "    for e in range(train_params[\"epochs\"]):\n",
        "        print(\"Epoch n\" + str(e))\n",
        "        train_loss, train_time_cost = run_epoch(e, trainload, device)\n",
        "        val_loss, val_time_cost,pred, label, mask = eval(valload, False, device)\n",
        "        train_loss = (train_loss * train_params['batch_size']) / len(trainload)\n",
        "        val_loss = (val_loss * train_params['batch_size']) / len(valload)\n",
        "        print('TRAIN {}\\t{} secs\\n'.format(train_loss, train_time_cost))\n",
        "        with open(\"v_behrt_log_train.txt\", 'a') as f:\n",
        "            f.write(\"Epoch n\" + str(e) + '\\n TRAIN {}\\t{} secs\\n'.format(train_loss, train_time_cost))\n",
        "            f.write('EVAL {}\\t{} secs\\n'.format(val_loss, val_time_cost) + '\\n\\n\\n')\n",
        "        print('EVAL {}\\t{} secs\\n'.format(val_loss, val_time_cost))\n",
        "        if val_loss < best_val:\n",
        "            print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "            model_to_save = behrt.module if hasattr(behrt, 'module') else behrt\n",
        "            save_model(model_to_save.state_dict(), '/content/drive/My Drive/GANBEHRT/models/v_behrt')\n",
        "            best_val = val_loss\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "def eval(_valload, saving, device):\n",
        "    tr_loss = 0\n",
        "    tr_g_loss = 0\n",
        "    tr_d_un = 0\n",
        "    tr_d_sup = 0\n",
        "    temp_loss = 0\n",
        "    start = time.time()\n",
        "    behrt.eval()\n",
        "    if saving:\n",
        "        with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_preds.csv\", 'w') as f:\n",
        "            f.write('')\n",
        "        with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_labels.csv\", 'w') as f:\n",
        "            f.write('')\n",
        "        with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_masks.csv\", 'w') as f:\n",
        "            f.write('')\n",
        "    for step, data in enumerate(_valload):\n",
        "        optim_behrt.zero_grad()\n",
        "\n",
        "        batched_data = Batch()\n",
        "        graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
        "        graph_batch = graph_batch.to(device)\n",
        "        nodes = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        edge_index_readout = graph_batch.edge_index\n",
        "        edge_attr = graph_batch.edge_attr\n",
        "        batch = graph_batch.batch\n",
        "        age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
        "        time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
        "        delta_ids = torch.reshape(graph_batch.delta, [graph_batch.delta.shape[0] // 50, 50])\n",
        "        type_ids = torch.reshape(graph_batch.adm_type, [graph_batch.adm_type.shape[0] // 50, 50])\n",
        "        posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
        "        attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
        "        attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
        "        los = torch.reshape(graph_batch.los, [graph_batch.los.shape[0] // 50, 50])\n",
        "\n",
        "        labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
        "        masks = torch.reshape(graph_batch.mask, [graph_batch.mask.shape[0] // 50, 50])[:, 0]\n",
        "        loss, logits = behrt(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids,delta_ids,type_ids,posi_ids,attMask, labels, masks, los)\n",
        "\n",
        "        if saving:\n",
        "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_preds.csv\", 'a') as f:\n",
        "                pd.DataFrame(logits.detach().cpu().numpy()).to_csv(f, header=False)\n",
        "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_labels.csv\", 'a') as f:\n",
        "                pd.DataFrame(labels.detach().cpu().numpy()).to_csv(f, header=False)\n",
        "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_masks.csv\", 'a') as f:\n",
        "                pd.DataFrame(masks.detach().cpu().numpy()).to_csv(f, header=False)\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        del loss\n",
        "\n",
        "    print(\"TOTAL LOSS\", (tr_loss * train_params['batch_size']) / len(_valload))\n",
        "\n",
        "    cost = time.time() - start\n",
        "    return tr_loss, cost, logits, labels, masks\n",
        "\n",
        "#%%\n",
        "\n",
        "def save_model(_model_dict, file_name):\n",
        "    torch.save(_model_dict, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Aw2Ou0soUIMV"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters())\n",
        "count_parameters(behrt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wm1FYyysUMFo"
      },
      "outputs": [],
      "source": [
        "pretrained_dict = torch.load(\"/content/drive/My Drive/GANBEHRT/models/v_behrt_pre_graph_nam_vtpmnp_7k\", map_location=train_params['device'])\n",
        "model_dict = behrt.state_dict()\n",
        "# 1. filter out unnecessary keys\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "# 2. overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "# 3. load the new state dict\n",
        "behrt.load_state_dict(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htLGX13DUSvw"
      },
      "outputs": [],
      "source": [
        "print(train_params['max_len_seq'])\n",
        "if train_params['action'] == 'train' or train_params['action'] == 'resume':\n",
        "    trainload = GraphLoader(GDSet(trainDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
        "    valload = GraphLoader(GDSet(valDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
        "\n",
        "    train_loss, val_loss = train(trainload, valload, train_params['device'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}